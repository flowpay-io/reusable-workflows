on:
  workflow_call:
    inputs:
      DATABRICKS_HOST:
        type: string
      SEGMENT_NAME:
        type: string


jobs:
  prod-pipeline:
    runs-on: ubuntu-latest
    name: Generate pipeline
    env:
      environment: DEV
      DATABRICKS_HOST: ${{ inputs.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_DEV_PAT }} 
    steps: 
      - name: Install Databricks CLI
        uses: microsoft/install-databricks-cli@v1.0.0
      
      - uses: actions/checkout@v3

      - name: Create location for flowpaycommon
        run: mkdir flowpaycommon

      - name: Checkout flowpaycommon repo
        uses: actions/checkout@v3
        with:
          repository: flowpay-io/flowpaycommon
          token: ${{ secrets.CICD_REPO_ACCESS_TOKEN }}
          path: ./flowpaycommon

      - name: Copy files from flowpaycommon
        run: |
          cp -r ./flowpaycommon/flowpaycommon/pipelines/. ./pipelines/

      - name: Set ssh for private flowpay repo
        uses: webfactory/ssh-agent@v0.5.4
        with:
          ssh-private-key: |
            ${{ secrets.SSH_PRIVATE_KEY_UTILS }}
      - name: Install packages from private flowpay repo
        run: pip install git+ssh://git@github.com/flowpay-io/flowpaycommon.git

      - name: Generate pipeline config file
        run: python3 ./pipelines/targets_pipeline.py --environment ${{ env.environment }} --pipe_repo_name ${{ github.event.repository.name }} --pipe_branch_ref dev
      - name: Remove previous pipeline
        run: >
          for job_id in $(databricks jobs list \
            | grep "targets_${{ inputs.SEGMENT }}" \
            | awk '{print $1}')
          do
              databricks jobs delete --job-id $job_id
          done
      
      - name: Remove previous pipeline versions
        run: >
          for job_id in $(databricks jobs list \
            | grep "target_${{ inputs.SEGMENT }}" \
            | awk '{print $1}')
          do
            databricks jobs delete --job-id $job_id
          done
      
      - name: Generate new pipeline
        run: |
          databricks jobs configure --version=2.1
          databricks jobs create --json-file ./pipelines/${{ env.environment }}_targets.json